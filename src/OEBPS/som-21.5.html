<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Society of Mind</title>
    <link rel="stylesheet" href="stylesheet.css" type="text/css" />


</head>
<body>
<h2>21.5 automatism</h2>

    <p>
      How do higher-level agencies tell lower-level agents what to do? We
      might expect this problem to be harder for smaller agencies because
      they can understand so much less. However, smaller agencies also
      have fewer concerns and hence need fewer instructions. Indeed, the
      smallest agencies may need scarcely any messages at all. For
      example, there&#39;s little need to tell Get, Put, or Find what
      to <em>get,</em>
      <em>put,</em> or <em>find</em> &mdash; since each can exploit the
      outcome of the Look-for agency&#39;s activity. But how can Look-for
      know what to look for? We&#39;ll see the answer in a trick that
      makes the problem disappear. In ordinary life this trick is referred
      to by names like <em>expectation</em> or
      <em>context.</em></p>

    <p>
      Whenever someone says a word like <em>apple,</em> you find yourself
      peculiarly disposed to <em>notice</em> any apple in the present
      scene. Your eyes tend to turn in the direction of that apple, your
      arm will prepare itself to reach out in that direction, and your
      hand will be disposed to form the corresponding grasping shape. This
      is because many of your agencies have become immersed in
      the <em>context</em> produced by the agents directly involved with
      whatever subject was mentioned recently. Thus, the polyneme for the
      word <em>apple</em> will arouse certain states of agencies that
      represent an object&#39;s color, shape, and size, and these will
      automatically affect the Look-for agency &mdash; simply because that
      agency must have been formed in the first place to depend upon the
      states of our object-description agencies. Accordingly, we can
      assume that Look-for is part of a larger society that includes
      connections like these:</p>

    <img class="illus" src="./illus/ch21/21-5.png"/>


    <p>
      This diagram portrays an automatic <em>finding machine.</em> Whether
      an apple was actually seen, imagined, or suggested by naming it, the
      agents for Color, Shape, and Size will be set into states that
      correspond to <em>red, round, and apple-sized.</em> Accordingly,
      when Look-for becomes active, it cannot help but seek an object with
      those properties. Then, according to our diagram, once such a thing
      is found, its location will automatically be represented by the
      state of an agency called Place &mdash; again, because this is the
      environment within which Look-for grew. The same must be true of the
      agency Move-arm-to, which must also have grown in the context of
      some location-representing agency like Place. So when Move-arm-to is
      aroused, it will automatically tend to move the arm and hand toward
      that location without needing to be told. Thus, such an arrangement
      of agencies can carry out the entire apple-moving script with
      virtually no <em>general-purpose</em> communication at all.</p>

    <p>
      This could be one explanation of what we call <em>focus of mental
	attention.</em> Because the agency that represents locations has a
      limited capacity, whenever some object is seen or heard &mdash; or
      merely imagined &mdash; other agencies that share the same
      representation of location are likely to be forced to become
      engaged with the same object. Then this becomes the
      momentary <em>it</em> of one&#39;s immediate concern.</p>
</body>
</html>
